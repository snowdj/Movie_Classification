{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prototype Naive Bayes on TMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tmdb_genres_list = pickle.load(open('data/tmdb_genres_list.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pickle.load(open('data/tmdb_processed.pkl', \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies[130973:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = len(tmdb_genres_list)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = np.array([m['title'] for m in movies])\n",
    "plots = np.array([(m['overview'].replace(',', '')).replace('.', '') for m in movies])\n",
    "genres = np.array([m['genre_ids'] for m in movies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_df=0.95, min_df=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.random.choice(len(plots), replace=False, size=int(len(plots)*0.8))\n",
    "test_idx = set(range(len(plots))) - set(train_idx)\n",
    "test_idx = np.array(list(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cv.fit_transform(plots[train_idx])\n",
    "X_test = cv.transform(plots[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb=MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y[train_idx]\n",
    "Y_test = Y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24000x1958 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 764893 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall(gt,preds):\n",
    "    TP=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for i in range(len(gt)):\n",
    "        if gt[i] ==1 and preds[i] == 1:\n",
    "            TP+=1\n",
    "        elif gt[i] ==0 and preds[i] == 1:\n",
    "            FN+=1\n",
    "        elif gt[i] ==1 and preds[i] == 0:\n",
    "            FP+=1\n",
    "    if TP+FP==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/float(TP+FP)\n",
    "    if TP+FN==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=TP/float(TP+FN)\n",
    "    if TP + FP + FN == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * TP / float(2 * TP + FP + FN)\n",
    "    return precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(gt, preds):\n",
    "    err = 0\n",
    "    for i in range(len(gt)):\n",
    "        if (gt[i] ==0 and preds[i] == 1) or (gt[i] ==1 and preds[i] == 0):\n",
    "            err +=1\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, LabelPowerset = False, tfidf=False):\n",
    "    if tfidf:\n",
    "        X_tr = X_train_tfidf\n",
    "        X_te = X_test_tfidf\n",
    "    else:\n",
    "        X_tr = X_train\n",
    "        X_te = X_test\n",
    "    \n",
    "    model.fit(X_tr, Y_train)\n",
    "#     train_acc = model.score(X_tr,Y_train)\n",
    "#     test_acc = model.score(X_te,Y_test)\n",
    "    \n",
    "    if LabelPowerset:\n",
    "        train_pred = model.predict(X_tr).toarray()\n",
    "        test_pred = model.predict(X_te).toarray()\n",
    "    else:\n",
    "        train_pred = model.predict(X_tr)\n",
    "        test_pred = model.predict(X_te)\n",
    "\n",
    "    f1_micro_train = f1_score(Y_train, train_pred, average='micro') \n",
    "    f1_micro_test = f1_score(Y_test, test_pred, average='micro') \n",
    "    prec_micro_train = precision_score(Y_train, train_pred, average='micro') \n",
    "    prec_micro_test = precision_score(Y_test, test_pred, average='micro') \n",
    "    recs_micro_train = recall_score(Y_train, train_pred, average='micro') \n",
    "    recs_micro_test = recall_score(Y_test, test_pred, average='micro') \n",
    "        \n",
    "#     train_precs=[]\n",
    "#     train_recs=[]\n",
    "#     test_precs=[]\n",
    "#     test_recs=[]\n",
    "#     train_f1=[]\n",
    "#     test_f1=[]\n",
    "# #     train_h_loss = []\n",
    "# #     test_h_loss = []\n",
    "    \n",
    "#     for i in range(len(Y_test)):\n",
    "#         a,b,c=precision_recall(Y_train[i],train_pred[i])\n",
    "#         train_precs.append(a)\n",
    "#         train_recs.append(b)\n",
    "#         train_f1.append(c)\n",
    "#         a,b,c=precision_recall(Y_test[i],test_pred[i])\n",
    "#         test_precs.append(a)\n",
    "#         test_recs.append(b)\n",
    "#         test_f1.append(c)\n",
    "#         train_h_loss.append(hamming_loss(Y_train[i],train_pred[i]))\n",
    "#         test_h_loss.append(hamming_loss(Y_test[i],test_pred[i]))\n",
    "    \n",
    "    #print(\"Acc: {} (train)   {} (test)\".format(train_acc, test_acc))\n",
    "    print(\" Training Precision:{} Recall :{} f1 micro score:{}\" .format(prec_micro_train, recs_micro_train, f1_micro_train))\n",
    "    print(\" Test Precision:{} Recall :{} f1 micro score:{}\" .format(prec_micro_test, recs_micro_test, f1_micro_test))\n",
    "    #print(\" Training Hamming Loss:{} Test Hamming Loss :{}\" .format(np.mean(np.asarray(train_h_loss)/k), np.mean(np.asarray(test_h_loss)/k)))\n",
    "    \n",
    "    \n",
    "    #print(\"F1: {} (train)   {} (test)\".format(train_f1, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = OneVsRestClassifier(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.4022589858674283 Recall :0.6525951557093426 f1 micro score:0.4977223962456829\n",
      " Test Precision:0.3718623636224918 Recall :0.5711941872729404 f1 micro score:0.4504621072088725\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.7103494623655914 Recall :0.1272152850910185 f1 micro score:0.21578584734733458\n",
      " Test Precision:0.6779661016949152 Recall :0.11250439470291808 f1 micro score:0.1929842195195497\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.6236653183193033 Recall :0.5149390702572589 f1 micro score:0.5641110158876657\n",
      " Test Precision:0.5139323731997495 Recall :0.38474159146841674 f1 micro score:0.4400509349239327\n"
     ]
    }
   ],
   "source": [
    "nb2 = LabelPowerset(MultinomialNB())\n",
    "evaluate(nb2, LabelPowerset= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.6078927396913736 Recall :0.2169098841582669 f1 micro score:0.3197321151372688\n",
      " Test Precision:0.5852233676975945 Recall :0.19957810851986404 f1 micro score:0.29764921786244863\n"
     ]
    }
   ],
   "source": [
    "evaluate(nb2, LabelPowerset=True, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = OneVsRestClassifier(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Precision:0.07410833333333332 Recall :0.0885\n"
     ]
    }
   ],
   "source": [
    "evaluate(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Precision:0.08425 Recall :0.106\n"
     ]
    }
   ],
   "source": [
    "evaluate(rf, tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = OneVsRestClassifier(LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.7838359128757586 Recall :0.5013389499022115 f1 micro score:0.6115393085223519\n",
      " Test Precision:0.48796312105173295 Recall :0.33493495839681237 f1 micro score:0.3972202918693537\n"
     ]
    }
   ],
   "source": [
    "evaluate(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.7606759009138456 Recall :0.39822476305100046 f1 micro score:0.5227712604179011\n",
      " Test Precision:0.6052821128451381 Recall :0.2954412281729755 f1 micro score:0.39707040478815564\n"
     ]
    }
   ],
   "source": [
    "evaluate(svm, tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm2 = LabelPowerset(LinearSVC())\n",
    "evaluate(svm2, LabelPowerset=True)\n",
    "evaluate(svm2, LabelPowerset=True, tfidf= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "parameters = {'kernel':['linear'], 'C':[0.01, 0.1, 1.0]}\n",
    "gridCV = GridSearchCV(SVC(class_weight='balanced'), parameters, scoring=make_scorer(f1_score, average='micro'))\n",
    "classif = OneVsRestClassifier(gridCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = OneVsRestClassifier(LogisticRegressionCV())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Precision:0.6572777777777777 Recall :0.7179444444444444\n",
      " Test Precision:0.35959444444444444 Recall :0.4159555555555555\n",
      " Training Hamming Loss:0.018359649122807017 Test Hamming Loss :0.059219298245614026\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = OneVsRestClassifier(LinearSVC())\n",
    "svm.fit(X_train, Y_train)\n",
    "train_pred = svm.predict(X_train)\n",
    "test_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45951290881205586"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_train, train_pred, average='weighted')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35960226805818063"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_test, test_pred, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = OneVsRestClassifier(MultinomialNB())\n",
    "nb.fit(X_train_tfidf, Y_train)\n",
    "train_pred = nb.predict(X_train_tfidf)\n",
    "test_pred = nb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5511154694778132"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(Y_test, test_pred, average='micro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(Y_test, test_pred, average='micro')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
